[[Vision Transformers Need Registers]]
[[Vision Transformers Don't Need Trained Registers]]
[[When Attention Sink Emerges in Language Models]]
[[Efficient Streaming Language Models with Attention Sinks]]
[[FlexAttention]]
[[Memory Efficient Optimizers with 4-bit States]]
[[Large Batch Optimization for Deep Learning]]
[[FP4 All the Way]]
[[The Era of 1-bit LLMs]]
[[Context Parallelism for Scalable Million-Token Inference]]
[[Training Ultra Long Context Language Model With Fully Pipelined Distributed Transformer]]

[[Model Cards for Model Reporting]]

Write CLI for Connected Papers to Obsidian Import. Should have confirm/deny filter and display url for quick relevance check

