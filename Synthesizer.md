Rethinking Self-Attention in Transformer Models
https://arxiv.org/abs/2005.00743